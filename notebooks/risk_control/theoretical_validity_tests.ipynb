{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Binary classification risk control - Theoretical tests to validate implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Protocol description\n",
    "We test the theoretical guarantees of risk control in binary classification by using a logistic classifier and synthetic data. The aim is to evaluate the effectiveness of the BinaryClassificationController in maintaining a predefined risk level under different conditions.\n",
    "\n",
    "Each test case corresponds to a unique set of parameters. We repeat the experiment `n_repeat` times for each combination. The model remains the same across experiments, while the data is resampled for each repetition to account for variability.\n",
    "\n",
    "Each experiment consists of the following steps:  \n",
    "\n",
    "- **Calibrate the controller**  \n",
    "  - We use a **BinaryClassificationController**, which provides a list of lambda values intended to control the risk according to **LTT**.  \n",
    "\n",
    "- **Verify risk control**  \n",
    "  - Since the model is a known logistic model, we can compute the **theoretical risk** associated with each lambda value.  \n",
    "  - We then check whether each lambda value from LTT actually controls the risk.  \n",
    "  - If a lambda does not meet the risk guarantee, we count **one \"error\"**.  \n",
    "  - **Note:** every lambda value must individually control the risk â€” it is not enough for only some to succeed.  \n",
    "\n",
    "After repeating the experiment `n_repeat` times, we calculate the **proportion of errors**, which should remain below `delta` = 1 - `confidence_level`.\n",
    "\n",
    "# Results\n",
    "***TBC***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mapie.risk_control import precision, accuracy, recall, BinaryClassificationController\n",
    "from itertools import product\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple logistic classifier\n",
    "class LogisticClassifier:\n",
    "    \"\"\"Deterministic sigmoid-based binary classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, scale=1.0, threshold=0.5):\n",
    "        self.scale = scale\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def _get_prob(self, x):\n",
    "        \"\"\"Probability of class 1 for input x.\"\"\"\n",
    "        inf_, sup_ = 0.1, 1.0\n",
    "        return (sup_ - inf_) / (1 + np.exp(-self.scale * x)) + inf_\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probabilities [p(y=0), p(y=1)] for each sample in X.\"\"\"\n",
    "        probs = np.array([self._get_prob(x) for x in X])\n",
    "        return np.vstack([1 - probs, probs]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predicted class labels based on threshold.\"\"\"\n",
    "        probs = self.predict_proba(X)[:, 1]\n",
    "        return (probs >= self.threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [100, 5]  # size of the calibration set\n",
    "risk = [\n",
    "    {\"name\": \"precision\", \"risk\": precision},\n",
    "    {\"name\": \"recall\", \"risk\": recall},\n",
    "    {\"name\": \"accuracy\", \"risk\": accuracy},\n",
    "]\n",
    "predict_params =  [np.linspace(0, 0.99, 100), np.array([0.5])]\n",
    "target_level = [0.1, 0.9]\n",
    "confidence_level = [0.8, 0.2]\n",
    "\n",
    "n_repeats = 100\n",
    "invalid_experiment = False\n",
    "\n",
    "for combination in product(N, risk, predict_params, target_level, confidence_level):\n",
    "    N, risk, predict_params, target_level, confidence_level = combination\n",
    "    alpha = float(Decimal(\"1\") - Decimal(str(target_level))) # to avoid floating point issues\n",
    "    delta = float(Decimal(\"1\") - Decimal(str(confidence_level))) # to avoid floating point issues\n",
    "\n",
    "    clf = LogisticClassifier(scale=2.0, threshold=0.5)\n",
    "    nb_errors = 0  # number of iterations where the risk is not controlled (i.e., not all the valid thresholds found by LTT are actually valid)\n",
    "    total_nb_valid_params = 0\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "\n",
    "        X_calibrate, y_calibrate = make_classification(\n",
    "            n_samples=N,\n",
    "            n_features=1,\n",
    "            n_informative=1,\n",
    "            n_redundant=0,\n",
    "            n_repeated=0,\n",
    "            n_classes=2,\n",
    "            n_clusters_per_class=1,\n",
    "            weights=[0.5, 0.5],\n",
    "            flip_y=0,\n",
    "            random_state=None\n",
    "        )\n",
    "        X_calibrate = X_calibrate.squeeze()\n",
    "\n",
    "        controller = BinaryClassificationController(\n",
    "            predict_function=clf.predict_proba,\n",
    "            risk=risk[\"risk\"],\n",
    "            target_level=target_level,\n",
    "            confidence_level=confidence_level,\n",
    "        )\n",
    "        controller._predict_params = predict_params\n",
    "        controller.calibrate(X_calibrate, y_calibrate)\n",
    "        valid_parameters = controller.valid_predict_params\n",
    "        total_nb_valid_params += len(valid_parameters)\n",
    "\n",
    "        # In the following, we check that all the valid thresholds found by LTT actually control the risk.\n",
    "        # We sample a large test set and estimate the risk for each valid_parameters using the logistic classifier.\n",
    "        X_test, y_test = make_classification(\n",
    "            n_samples=100,\n",
    "            n_features=1,\n",
    "            n_informative=1,\n",
    "            n_redundant=0,\n",
    "            n_repeated=0,\n",
    "            n_classes=2,\n",
    "            n_clusters_per_class=1,\n",
    "            weights=[0.5, 0.5],\n",
    "            flip_y=0,\n",
    "            random_state=None\n",
    "        )\n",
    "        X_test = X_test.squeeze()\n",
    "        probs = clf.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # If no valid parameters found, risk is not controlled\n",
    "        if len(valid_parameters) >= 1:\n",
    "            for lambda_ in valid_parameters:\n",
    "                y_pred = (probs >= lambda_).astype(int)\n",
    "\n",
    "                if risk[\"risk\"] == precision:\n",
    "                    empirical_metric = precision_score(y_test, y_pred, zero_division=0)\n",
    "                elif risk[\"risk\"] == recall:\n",
    "                    empirical_metric = recall_score(y_test, y_pred, zero_division=0)\n",
    "                elif risk[\"risk\"] == accuracy:\n",
    "                    empirical_metric = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                # Check if the risk control fails\n",
    "                if risk[\"risk\"].higher_is_better:\n",
    "                    if empirical_metric <= target_level:\n",
    "                        nb_errors += 1\n",
    "                        break \n",
    "                else:\n",
    "                    if empirical_metric > target_level:\n",
    "                        nb_errors += 1\n",
    "                        break\n",
    "\n",
    "    print(f\"\\n{N=}, {risk['name']=}, {len(predict_params)=}, {target_level=}, {confidence_level=}\")\n",
    "\n",
    "    print(f\"Proportion of times the risk is not controlled: {nb_errors/n_repeats}\")\n",
    "    print(f\"Delta: {delta}\")\n",
    "    print(f\"Mean number of valid thresholds found per iteration: {int(np.round(total_nb_valid_params/n_repeats))}\")\n",
    "\n",
    "    if nb_errors/n_repeats <= delta:\n",
    "        print(\"Valid experiment\")\n",
    "    else:\n",
    "        print(\"Invalid experiment\")\n",
    "        invalid_experiment = True\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "if invalid_experiment:\n",
    "    print(\"Some experiments failed.\")\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
