{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Binary classification risk control - Theoretical tests to validate implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Protocol description\n",
    "We test the theoretical guarantees of risk control in binary classification by using a logistic classifier and synthetic data. The aim is to evaluate the effectiveness of the BinaryClassificationController in maintaining a predefined risk level under different conditions.\n",
    "\n",
    "Each test case corresponds to a unique set of parameters. We repeat the experiment `n_repeat` times for each combination. The model remains the same across experiments, while the data is resampled for each repetition to account for variability.\n",
    "\n",
    "Each experiment consists of the following steps:  \n",
    "\n",
    "- **Calibrate the controller**  \n",
    "  - We use a **BinaryClassificationController**, which provides a list of lambda values intended to control the risk according to **LTT**.  \n",
    "\n",
    "- **Verify risk control**  \n",
    "  - Since the model is a known logistic model, we can compute the **theoretical risk** associated with each lambda value.  \n",
    "  - We then check whether each lambda value from LTT actually controls the risk.  \n",
    "  - If a lambda does not meet the risk guarantee, we count **one \"error\"**.  \n",
    "  - **Note:** every lambda value must individually control the risk â€” it is not enough for only some to succeed.  \n",
    "\n",
    "After repeating the experiment `n_repeat` times, we calculate the **proportion of errors**, which should remain below `delta` = 1 - `confidence_level`.\n",
    "\n",
    "# Results\n",
    "The LTT procedure generally controls the risk as expected, with most experiments marked as valid. However, experiments are mainly invalidated when the target level is set too high (e.g. 0.9) or when the calibration set is too small, resulting in an insufficient number of valid thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "from sklearn.utils import check_random_state\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mapie.risk_control import precision, accuracy, recall, BinaryClassificationController\n",
    "from itertools import product\n",
    "from decimal import Decimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple logistic classifier\n",
    "class LogisticClassifier:\n",
    "    \"\"\"Deterministic sigmoid-based binary classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, scale=1.0, threshold=0.5):\n",
    "        self.scale = scale\n",
    "        self.threshold = threshold\n",
    "\n",
    "    def _get_prob(self, x):\n",
    "        \"\"\"Probability of class 1 for input x.\"\"\"\n",
    "        inf_, sup_ = 0.1, 1.0\n",
    "        return (sup_ - inf_) / (1 + np.exp(-self.scale * x)) + inf_\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return probabilities [p(y=0), p(y=1)] for each sample in X.\"\"\"\n",
    "        probs = np.array([self._get_prob(x) for x in X])\n",
    "        return np.vstack([1 - probs, probs]).T\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Return predicted class labels based on threshold.\"\"\"\n",
    "        probs = self.predict_proba(X)[:, 1]\n",
    "        return (probs >= self.threshold).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate logistic data\n",
    "def make_logistic_data(n_samples=200, scale=2.0, random_state=None):\n",
    "    rng = check_random_state(random_state)\n",
    "    X = rng.uniform(-3, 3, size=n_samples)\n",
    "    logits = scale * X\n",
    "    probs = 1 / (1 + np.exp(-logits))\n",
    "    y = rng.binomial(1, probs)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = [100, 5]  # size of the calibration set\n",
    "risk = [\n",
    "    {\"name\": \"precision\", \"risk\": precision},\n",
    "    {\"name\": \"recall\", \"risk\": recall},\n",
    "    {\"name\": \"accuracy\", \"risk\": accuracy},\n",
    "]\n",
    "predict_params =  [np.linspace(0, 0.99, 100), np.array([0.5])]\n",
    "target_level = [0.1, 0.9]\n",
    "confidence_level = [0.8, 0.2]\n",
    "\n",
    "n_repeats = 100\n",
    "invalid_experiment = False\n",
    "\n",
    "for combination in product(N, risk, predict_params, target_level, confidence_level):\n",
    "    N, risk, predict_params, target_level, confidence_level = combination\n",
    "    alpha = float(Decimal(\"1\") - Decimal(str(target_level))) # to avoid floating point issues\n",
    "    delta = float(Decimal(\"1\") - Decimal(str(confidence_level))) # to avoid floating point issues\n",
    "\n",
    "    clf = LogisticClassifier(scale=2.0, threshold=0.5)\n",
    "    nb_errors = 0  # number of iterations where the risk is not controlled (i.e., not all the valid thresholds found by LTT are actually valid)\n",
    "    total_nb_valid_params = 0\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "\n",
    "        # X_calibrate, y_calibrate = make_classification(\n",
    "        #     n_samples=N,\n",
    "        #     n_features=1,\n",
    "        #     n_informative=1,\n",
    "        #     n_redundant=0,\n",
    "        #     n_repeated=0,\n",
    "        #     n_classes=2,\n",
    "        #     n_clusters_per_class=1,\n",
    "        #     weights=[0.5, 0.5],\n",
    "        #     flip_y=0,\n",
    "        #     random_state=None\n",
    "        # )\n",
    "        # X_calibrate = X_calibrate.squeeze()\n",
    "\n",
    "        X_calibrate, y_calibrate = make_logistic_data(n_samples=N, scale=2.0, random_state=None)\n",
    "\n",
    "        controller = BinaryClassificationController(\n",
    "            predict_function=clf.predict_proba,\n",
    "            risk=risk[\"risk\"],\n",
    "            target_level=target_level,\n",
    "            confidence_level=confidence_level,\n",
    "        )\n",
    "        controller._predict_params = predict_params\n",
    "        controller.calibrate(X_calibrate, y_calibrate)\n",
    "        valid_parameters = controller.valid_predict_params\n",
    "        total_nb_valid_params += len(valid_parameters)\n",
    "\n",
    "        # In the following, we check that all the valid thresholds found by LTT actually control the risk.\n",
    "        # We sample a large test set and estimate the risk for each valid_parameters using the logistic classifier.\n",
    "        X_test, y_test = make_classification(\n",
    "            n_samples=100,\n",
    "            n_features=1,\n",
    "            n_informative=1,\n",
    "            n_redundant=0,\n",
    "            n_repeated=0,\n",
    "            n_classes=2,\n",
    "            n_clusters_per_class=1,\n",
    "            weights=[0.5, 0.5],\n",
    "            flip_y=0,\n",
    "            random_state=None\n",
    "        )\n",
    "        X_test = X_test.squeeze()\n",
    "        probs = clf.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        # If no valid parameters found, risk is not controlled\n",
    "        if len(valid_parameters) >= 1:\n",
    "            for lambda_ in valid_parameters:\n",
    "                y_pred = (probs >= lambda_).astype(int)\n",
    "\n",
    "                if risk[\"risk\"] == precision:\n",
    "                    empirical_metric = precision_score(y_test, y_pred, zero_division=0)\n",
    "                elif risk[\"risk\"] == recall:\n",
    "                    empirical_metric = recall_score(y_test, y_pred, zero_division=0)\n",
    "                elif risk[\"risk\"] == accuracy:\n",
    "                    empirical_metric = accuracy_score(y_test, y_pred)\n",
    "\n",
    "                # Check if the risk control fails\n",
    "                if risk[\"risk\"].higher_is_better:\n",
    "                    if empirical_metric <= target_level:\n",
    "                        nb_errors += 1\n",
    "                        break \n",
    "                else:\n",
    "                    if empirical_metric > target_level:\n",
    "                        nb_errors += 1\n",
    "                        break\n",
    "\n",
    "    print(f\"\\n{N=}, {risk['name']=}, {len(predict_params)=}, {target_level=}, {confidence_level=}\")\n",
    "\n",
    "    print(f\"Proportion of times the risk is not controlled: {nb_errors/n_repeats}\")\n",
    "    print(f\"Delta: {delta}\")\n",
    "    print(f\"Mean number of valid thresholds found per iteration: {int(np.round(total_nb_valid_params/n_repeats))}\")\n",
    "\n",
    "    if nb_errors/n_repeats <= delta:\n",
    "        print(\"Valid experiment\")\n",
    "    else:\n",
    "        print(\"Invalid experiment\")\n",
    "        invalid_experiment = True\n",
    "\n",
    "print(\"\\n\\n\\n\")\n",
    "if invalid_experiment:\n",
    "    print(\"Some experiments failed.\")\n",
    "else:\n",
    "    print(\"All good!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "# Analysis of the distribution of the supremum of the risk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment parameters\n",
    "risk = [\n",
    "    {\"name\": \"precision\", \"risk\": precision, \"risk_score\": precision_score},\n",
    "    {\"name\": \"recall\", \"risk\": recall, \"risk_score\": recall_score},\n",
    "    {\"name\": \"accuracy\", \"risk\": accuracy, \"risk_score\": accuracy_score},\n",
    "]\n",
    "target_level = [0.1, 0.9]\n",
    "confidence_level = [0.8, 0.2]\n",
    "\n",
    "# Fixed classifier\n",
    "clf = LogisticClassifier(scale=2.0, threshold=0.5)\n",
    "\n",
    "# Number of experiment repetitions\n",
    "n_repeats = 100\n",
    "\n",
    "# Storage list\n",
    "results = []\n",
    "\n",
    "for rsk, tgt, conf in product(risk, target_level, confidence_level):\n",
    "    print(f\"\\nRunning {rsk['name']=}, {tgt=}, {conf=}\")\n",
    "\n",
    "    min_metrics = []\n",
    "    best_lambdas = []\n",
    "\n",
    "    for _ in range(n_repeats):\n",
    "        # Data generation\n",
    "        # X_calibrate, y_calibrate = make_classification(\n",
    "        #     n_samples=1000, n_features=1, n_informative=1, n_redundant=0,\n",
    "        #     n_classes=2, n_clusters_per_class=1, random_state=None\n",
    "        # )\n",
    "        # X_calibrate = X_calibrate.squeeze()\n",
    "        X_calibrate, y_calibrate = make_logistic_data(\n",
    "            n_samples=1000, scale=2.0, random_state=None\n",
    "        )\n",
    "\n",
    "        # X_test, y_test = make_classification(\n",
    "        #     n_samples=1000, n_features=1, n_informative=1, n_redundant=0,\n",
    "        #     n_classes=2, n_clusters_per_class=1, random_state=None\n",
    "        # )\n",
    "        # X_test = X_test.squeeze()\n",
    "\n",
    "        X_test, y_test = make_logistic_data(\n",
    "            n_samples=1000, scale=2.0, random_state=None\n",
    "        )\n",
    "\n",
    "        # Calibration\n",
    "        controller = BinaryClassificationController(\n",
    "            predict_function=clf.predict_proba,\n",
    "            risk=rsk[\"risk\"],\n",
    "            target_level=tgt,\n",
    "            confidence_level=conf,\n",
    "        )\n",
    "        controller._predict_params = np.linspace(0, 0.99, 100)\n",
    "        controller.calibrate(X_calibrate, y_calibrate)\n",
    "        valid_parameters = controller.valid_predict_params\n",
    "\n",
    "        probs = clf.predict_proba(X_test)[:, 1]\n",
    "\n",
    "        empirical_metric_list = []\n",
    "        if len(valid_parameters) > 0:\n",
    "            for lambda_ in valid_parameters:\n",
    "                y_pred = (probs >= lambda_).astype(int)\n",
    "                if rsk[\"risk\"] == accuracy:\n",
    "                    empirical_metric = rsk[\"risk_score\"](y_test, y_pred)\n",
    "                else:\n",
    "                    empirical_metric = rsk[\"risk_score\"](y_test, y_pred, zero_division=0)\n",
    "                empirical_metric_list.append(empirical_metric)\n",
    "\n",
    "            empirical_metric_list = np.array(empirical_metric_list)\n",
    "            min_idx = np.argmin(empirical_metric_list)\n",
    "            min_metrics.append(empirical_metric_list[min_idx])\n",
    "            best_lambdas.append(valid_parameters[min_idx])\n",
    "        else:\n",
    "            min_metrics.append(-1)\n",
    "            best_lambdas.append(-1)\n",
    "            best_lambdas.append(-1)\n",
    "            best_lambdas.append(-1)\n",
    "            best_lambdas.append(-1)\n",
    "\n",
    "    results.append({\n",
    "        \"risk\": rsk[\"name\"],\n",
    "        \"target\": tgt,\n",
    "        \"confidence\": conf,\n",
    "        \"min_metrics\": np.array(min_metrics),\n",
    "        \"best_lambdas\": np.array(best_lambdas)\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting results\n",
    "risks = [\"precision\", \"recall\", \"accuracy\"]\n",
    "unique_targets = sorted(list(set(res[\"target\"] for res in results)))\n",
    "unique_confidences = sorted(list(set(res[\"confidence\"] for res in results)))\n",
    "\n",
    "for risk_name in risks:\n",
    "    res_risk = [r for r in results if r[\"risk\"] == risk_name]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(14, 7), sharex=False)\n",
    "    fig.suptitle(f\"{risk_name.capitalize()} â€” Risk Control Visualization\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "    # Updated explanatory legend text (under title)\n",
    "    legend_text = (\n",
    "        \"Shaded area: upper (1âˆ’Î´)-tail of sup R(Î»). \"\n",
    "        \"Risk controlled if at least the (1âˆ’Î´) percentile lies below Î±.\"\n",
    "    )\n",
    "    fig.text(0.5, 0.93, legend_text, ha=\"center\", va=\"bottom\", fontsize=9, color=\"gray\")\n",
    "\n",
    "    idx = 0\n",
    "    for t in unique_targets:\n",
    "        for c in unique_confidences:\n",
    "            ax_top = axes[0, idx]\n",
    "            ax_bottom = axes[1, idx]\n",
    "\n",
    "            res_conf = [r for r in res_risk if r[\"target\"] == t and r[\"confidence\"] == c]\n",
    "            if len(res_conf) == 0:\n",
    "                # Always show an empty top plot with legend\n",
    "                ax_top.hist([], bins=30, color=\"steelblue\", alpha=0.7, edgecolor=\"white\")\n",
    "                ax_top.axvline(1 - t, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "                ax_top.axvspan(0, 0, color=\"salmon\", alpha=0.3)  # empty shaded\n",
    "                ax_top.set_title(f\"target={t}, conf={c}\", fontsize=10)\n",
    "                ax_top.set_ylabel(\"Freq.\")\n",
    "                ax_top.legend(\n",
    "                    [\"$\\\\alpha = 1 - target$\", \"upper Î´-tail (uncontrolled)\"],\n",
    "                    loc=\"upper center\", frameon=False, fontsize=8\n",
    "                )\n",
    "\n",
    "                # Empty bottom plot\n",
    "                ax_bottom.hist([], bins=30, color=\"darkorange\", alpha=0.7, edgecolor=\"white\")\n",
    "                ax_bottom.set_xlabel(\"Î» (threshold)\")\n",
    "                ax_bottom.set_ylabel(\"Freq.\")\n",
    "                idx += 1\n",
    "                continue\n",
    "\n",
    "            r = res_conf[0]\n",
    "            sup_r = 1 - r[\"min_metrics\"]\n",
    "            lambdas = r[\"best_lambdas\"]\n",
    "\n",
    "            clean_sup_r = sup_r[~np.isnan(sup_r)]\n",
    "            if clean_sup_r.size > 0:\n",
    "                ax_top.hist(clean_sup_r, bins=30, alpha=0.7, color=\"steelblue\", edgecolor=\"white\")\n",
    "                alpha_line = 1 - t\n",
    "                ax_top.axvline(alpha_line, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "                delta = 1 - c\n",
    "                perc = (1 - delta) * 100\n",
    "                threshold = np.percentile(clean_sup_r, perc)\n",
    "                ax_top.axvspan(threshold, np.max(clean_sup_r), color=\"salmon\", alpha=0.3)\n",
    "            else:\n",
    "                # Show empty histogram if no data\n",
    "                ax_top.hist([], bins=30, color=\"steelblue\", alpha=0.7, edgecolor=\"white\")\n",
    "                ax_top.axvline(1 - t, color=\"red\", linestyle=\"--\", linewidth=2)\n",
    "                ax_top.axvspan(0, 0, color=\"salmon\", alpha=0.3)\n",
    "\n",
    "            ax_top.set_title(f\"target={t}, conf={c}\", fontsize=10)\n",
    "            ax_top.set_ylabel(\"Freq.\")\n",
    "            ax_top.legend(\n",
    "                [\"$\\\\alpha = 1 - target$\", \"upper (1-Î´)-tail\"],\n",
    "                loc=\"upper center\", frameon=False, fontsize=8\n",
    "            )\n",
    "\n",
    "            # Bottom: Î» histogram\n",
    "            clean_lambda = lambdas[~np.isnan(lambdas)]\n",
    "            if clean_lambda.size > 0:\n",
    "                ax_bottom.hist(clean_lambda, bins=30, alpha=0.7, color=\"darkorange\", edgecolor=\"white\")\n",
    "            else:\n",
    "                ax_bottom.hist([], bins=30, color=\"darkorange\", alpha=0.7, edgecolor=\"white\")\n",
    "\n",
    "            ax_bottom.set_xlabel(\"Î» (threshold)\")\n",
    "            ax_bottom.set_ylabel(\"Freq.\")\n",
    "            idx += 1\n",
    "\n",
    "    # Hide unused subplots\n",
    "    for j in range(idx, 4):\n",
    "        axes[0, j].axis(\"off\")\n",
    "        axes[1, j].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.9])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "X_calibrate, y_calibrate = make_classification(\n",
    "    n_samples=10000,\n",
    "    n_features=1,\n",
    "    n_informative=1,\n",
    "    n_redundant=0,\n",
    "    n_repeated=0,\n",
    "    n_classes=2,\n",
    "    n_clusters_per_class=1,\n",
    "    weights=[0.5, 0.5],\n",
    "    flip_y=0,\n",
    "    random_state=None\n",
    ")\n",
    "X_calibrate = X_calibrate.squeeze()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.scatter(X_calibrate[y_calibrate == 0], np.zeros_like(X_calibrate[y_calibrate == 0]),\n",
    "            color=\"blue\", alpha=0.7, label=\"Class 0\")\n",
    "plt.scatter(X_calibrate[y_calibrate == 1], np.ones_like(X_calibrate[y_calibrate == 1]),\n",
    "            color=\"red\", alpha=0.7, label=\"Class 1\")\n",
    "\n",
    "# Small jitter for visibility\n",
    "plt.yticks([0, 1], [\"Class 0\", \"Class 1\"])\n",
    "plt.xlabel(\"Feature value (X)\")\n",
    "plt.title(\"1D Binary Classification Dataset\")\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_logistic_data(n_samples=1000, scale=2.0, random_state=None)\n",
    "clf = LogisticClassifier(scale=2.0, threshold=0.5)\n",
    "\n",
    "order = np.argsort(X)\n",
    "X_sorted = X[order]\n",
    "probs_sorted = clf._get_prob(X_sorted)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(7, 3))\n",
    "plt.scatter(X, y, c=y, cmap=\"coolwarm\", alpha=0.5, edgecolor=\"none\", label=\"Samples\")\n",
    "plt.plot(X_sorted, probs_sorted, color=\"black\", linewidth=2)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv-dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
