{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Estimating prediction sets on the Cifar10 dataset\n",
    "The goal of this notebook is to present how to use :class:`mapie.classification.MapieClassifier` on an object classification task. We will build prediction sets for images and study the marginal and conditional coverages."
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/scikit-learn-contrib/MAPIE/blob/master/notebooks/classification/Cifar10.ipynb)\n"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### What is done in this tutorial ?\n",
    "\n",
    "> - **Cifar10 dataset** : 10 classes (horse, dog, cat, frog, deer, bird, airplane, truck, ship, automobile)\n",
    "\n",
    "> - Use :class:`mapie.classification.MapieClassifier` to compare the prediction sets estimated by several conformal methods on the Cifar10 dataset. \n",
    "\n",
    "> - Train a small CNN to predict the image class\n",
    "\n",
    "> - Create a custom class `TensorflowToMapie` to resolve adherence problems between Tensorflow and Mapie\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Tutorial preparation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "install_mapie = True\n",
    "if install_mapie:\n",
    "    !pip install mapie"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import random\n",
    "import warnings\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as tfk\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Dropout, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics._plot.confusion_matrix import ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "from mapie.metrics.classification import classification_coverage_score\n",
    "from mapie.classification import _MapieClassifier\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "# %load_ext pycodestyle_magic"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "SPACE_BETWEEN_LABELS = 2.5\n",
    "SPACE_IN_SUBPLOTS = 4.0\n",
    "FONT_SIZE = 18\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 1. Data loading"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "The Cifar10 dataset is downloaded from the `Tensorflow Datasets` library. The training set is then splitted into a training, validation and a calibration set which will be used as follow:\n",
    "\n",
    "> - **Training set**: used to train our neural network.\n",
    "> - **Validation set**: used to check that our model is not overfitting.\n",
    "> - **Calibration set**: used to calibrate the conformal scores in :class:`mapie.classification.MapieClassifier`"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def train_valid_calib_split(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    calib_size: float = .1,\n",
    "    val_size: float = .33,\n",
    "    random_state: int = 42\n",
    "\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Create calib and valid datasets from the train dataset.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray of shape (n_samples, width, height, n_channels)\n",
    "        Images of the dataset.\n",
    "    \n",
    "    y: np.ndarray of shape (n_samples, 1):\n",
    "        Label of each image.\n",
    "    \n",
    "    calib_size: float\n",
    "        Percentage of the dataset X to use as calibration set.\n",
    "    \n",
    "    val_size: float\n",
    "        Percentage of the dataset X (minus the calibration set)\n",
    "        to use as validation set.\n",
    "    \n",
    "    random_state: int\n",
    "        Random state to use to split the dataset.\n",
    "        \n",
    "        By default 42.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray, np.ndarray]\n",
    "    of shapes: \n",
    "    (n_samples * (1 - calib_size) * (1 - val_size), width, height, n_channels),\n",
    "    (n_samples * calib_size, width, height, n_channels),\n",
    "    (n_samples * (1 - calib_size) * val_size, width, height, n_channels),\n",
    "    (n_samples * (1 - calib_size) * (1 - val_size), 1),\n",
    "    (n_samples * calib_size, 1),\n",
    "    (n_samples * (1 - calib_size) * val_size, 1).\n",
    "    \n",
    "    \"\"\"\n",
    "    X_train, X_calib, y_train, y_calib = train_test_split(\n",
    "        X, y,\n",
    "        test_size=calib_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train, y_train,\n",
    "        test_size=val_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "    return X_train, X_calib, X_val, y_train, y_calib, y_val\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def load_data() -> Tuple[\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "    Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "    List\n",
    "]:\n",
    "    \"\"\"\n",
    "    Load cifar10 Dataset and return train, valid, calib, test datasets\n",
    "    and the names of the labels\n",
    "    \n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Tuple[\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray],\n",
    "        List\n",
    "    ]\n",
    "    \"\"\"\n",
    "    dataset, info = tfds.load(\n",
    "        \"cifar10\",\n",
    "        batch_size=-1,\n",
    "        as_supervised=True,\n",
    "        with_info=True\n",
    "    )\n",
    "    label_names = info.features['lac'].names\n",
    "\n",
    "    dataset = tfds.as_numpy(dataset)\n",
    "    X_train, y_train = dataset['train']\n",
    "    X_test, y_test = dataset['test']\n",
    "    X_train, X_calib, X_val, y_train, y_calib, y_val = train_valid_calib_split(\n",
    "        X_train,\n",
    "        y_train\n",
    "    )\n",
    "\n",
    "    X_train = X_train/255.\n",
    "    X_val = X_val/255.\n",
    "\n",
    "    X_calib = X_calib/255.\n",
    "    X_test = X_test/255.\n",
    "\n",
    "    y_train_cat = tf.keras.utils.to_categorical(y_train)\n",
    "    y_val_cat = tf.keras.utils.to_categorical(y_val)\n",
    "    y_calib_cat = tf.keras.utils.to_categorical(y_calib)\n",
    "    y_test_cat = tf.keras.utils.to_categorical(y_test)\n",
    "\n",
    "    train_set = (X_train, y_train, y_train_cat)\n",
    "    val_set = (X_val, y_val, y_val_cat)\n",
    "    calib_set = (X_calib, y_calib, y_calib_cat)\n",
    "    test_set = (X_test, y_test, y_test_cat)\n",
    "\n",
    "    return train_set, val_set, calib_set, test_set, label_names\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def inspect_images(\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    num_images: int, \n",
    "    label_names: List\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Load a sample of the images to check that images\n",
    "    are well loaded.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray of shape (n_samples, width, height, n_channels)\n",
    "        Set of images from which the sample will be taken.\n",
    "    \n",
    "    y: np.ndarray of shape (n_samples, 1)\n",
    "        Labels of the iamges of X.\n",
    "    \n",
    "    num_images: int\n",
    "        Number of images to plot.\n",
    "        \n",
    "    label_names: List\n",
    "        Names of the different labels\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    _, ax = plt.subplots(\n",
    "        nrows=1,\n",
    "        ncols=num_images,\n",
    "        figsize=(2*num_images, 2)\n",
    "    )\n",
    "\n",
    "    indices = random.sample(range(len(X)), num_images)\n",
    "\n",
    "    for i, indice in enumerate(indices):\n",
    "        ax[i].imshow(X[indice])\n",
    "        ax[i].set_title(label_names[y[indice]])\n",
    "        ax[i].axis(\"off\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "train_set, val_set, calib_set, test_set, label_names = load_data()\n",
    "(X_train, y_train, y_train_cat) = train_set \n",
    "(X_val, y_val, y_val_cat) = val_set \n",
    "(X_calib, y_calib, y_calib_cat) = calib_set \n",
    "(X_test, y_test, y_test_cat) = test_set \n",
    "inspect_images(X=X_train, y=y_train, num_images=8, label_names=label_names)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 2. Definition and training of the the neural network"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "We define a simple convolutional neural network with the following architecture : \n",
    "\n",
    "> - 2 blocks of Convolution/Maxpooling\n",
    "> - Flatten the images\n",
    "> - 3 Dense layers\n",
    "> - The output layer with 10 neurons, corresponding to our 10 classes\n",
    "\n",
    "This simple architecture, based on the VGG16 architecture with its succession of convolutions and maxpooling aims at achieving a reasonable accuracy score and a fast training. The objective here is not to obtain a perfect classifier.\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_model(\n",
    "    input_shape: Tuple, loss: tfk.losses,\n",
    "    optimizer: tfk.optimizers, metrics: List[str]\n",
    ") -> Sequential:\n",
    "    \"\"\"\n",
    "    Compile CNN model.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    input_shape: Tuple\n",
    "        Size of th input images.\n",
    "    \n",
    "    loss: tfk.losses\n",
    "        Loss to use to train the model.\n",
    "    \n",
    "    optimizer: tfk.optimizer\n",
    "        Optimizer to use to train the model.\n",
    "    \n",
    "    metrics: List[str]\n",
    "        Metrics to use evaluate model training.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    Sequential\n",
    "    \"\"\"\n",
    "    model = Sequential([\n",
    "        Conv2D(input_shape=input_shape, filters=16, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(input_shape=input_shape, filters=32, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Conv2D(input_shape=input_shape, filters=64, kernel_size=(3, 3), activation='relu', padding='same'),\n",
    "        MaxPooling2D(pool_size=(2, 2)),\n",
    "        Flatten(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(10, activation='softmax'),\n",
    "    ])\n",
    "    model.compile(loss=loss, optimizer=optimizer, metrics=metrics)\n",
    "    return model"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 3. Training the algorithm with a custom class called `TensorflowToMapie`\n",
    "\n",
    "As MAPIE asks for a model with `fit`, `predict_proba`, `predict` class attributes and the information about whether or not the model is fitted."
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TensorflowToMapie():\n",
    "    \"\"\"\n",
    "    Class that aimes to make compatible a tensorflow model\n",
    "    with MAPIE. To do so, this class create fit, predict,\n",
    "    predict_proba and _sklearn_is_fitted_ attributes to the model.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self) -> None:\n",
    "        self.pred_proba = None\n",
    "        self.trained_ = False\n",
    "        \n",
    "\n",
    "    def fit(\n",
    "        self, model: Sequential,\n",
    "        X_train: np.ndarray, y_train: np.ndarray,\n",
    "        X_val: np.ndarray, y_val: np.ndarray\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Train the keras model.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        model: Sequential\n",
    "            Model to train.\n",
    "            \n",
    "        X_train: np.ndarray of shape (n_sample_train, width, height, n_channels)\n",
    "            Training images.\n",
    "        \n",
    "        y_train: np.ndarray of shape (n_samples_train, n_labels)\n",
    "            Training labels.\n",
    "        \n",
    "        X_val: np.ndarray of shape (n_sample_val, width, height, n_channels)\n",
    "            Validation images.\n",
    "        \n",
    "        y_val: np.ndarray of shape (n_samples_val, n_labels)\n",
    "            Validation labels.\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        early_stopping_monitor = EarlyStopping(\n",
    "                    monitor='val_loss',\n",
    "                    min_delta=0,\n",
    "                    patience=10,\n",
    "                    verbose=0,\n",
    "                    mode='auto',\n",
    "                    baseline=None,\n",
    "                    restore_best_weights=True\n",
    "                    )\n",
    "        model.fit(\n",
    "                    X_train, y_train, \n",
    "                    batch_size=64, \n",
    "                    validation_data=(X_val, y_val), \n",
    "                    epochs=20, callbacks=[early_stopping_monitor]\n",
    "                )\n",
    "        \n",
    "        self.model = model\n",
    "        self.trained_ = True\n",
    "        self.classes_ = np.arange(model.layers[-1].units)\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Returns the predicted probabilities of the images in X.\n",
    "        \n",
    "        Paramters:\n",
    "        X: np.ndarray of shape (n_sample, width, height, n_channels)\n",
    "            Images to predict.\n",
    "        \n",
    "        Returns:\n",
    "        np.ndarray of shape (n_samples, n_labels)\n",
    "        \"\"\"\n",
    "        preds = self.model.predict(X)\n",
    "          \n",
    "        return preds\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Give the label with the maximum softmax for each image.\n",
    "        \n",
    "        Parameters\n",
    "        ---------\n",
    "        X: np.ndarray of shape (n_sample, width, height, n_channels)\n",
    "            Images to predict\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        np.ndarray of shape (n_samples, 1)\n",
    "        \"\"\"\n",
    "        pred_proba = self.predict_proba(X)\n",
    "        pred = (pred_proba == pred_proba.max(axis=1)[:, None]).astype(int)\n",
    "        return pred\n",
    "\n",
    "    def __sklearn_is_fitted__(self):\n",
    "        if self.trained_:\n",
    "            return True\n",
    "        else:\n",
    "            return False"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "model = get_model(\n",
    "    input_shape=(32, 32, 3), \n",
    "    loss=CategoricalCrossentropy(), \n",
    "    optimizer=Adam(), \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "cirfar10_model = TensorflowToMapie()\n",
    "cirfar10_model.fit(model, X_train, y_train_cat, X_val, y_val_cat)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_true = label_binarize(y=y_test, classes=np.arange(max(y_test)+1))\n",
    "y_pred_proba = cirfar10_model.predict_proba(X_test)\n",
    "y_pred = cirfar10_model.predict(X_test)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 4. Prediction of the prediction sets"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We will now estimate the prediction sets with the five conformal methods implemented in :class:`mapie.classification.MapieClassifier` for a range of confidence levels between 0 and 1. "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "method_params = {\n",
    "    \"naive\": (\"naive\", False),\n",
    "    \"label\": (\"label\", False),\n",
    "    \"aps\": (\"aps\", True),\n",
    "    \"random_aps\": (\"aps\", \"randomized\"),\n",
    "    \"top_k\": (\"top_k\", False)\n",
    "}\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "y_preds, y_pss = {}, {}\n",
    "alphas = np.arange(0.01, 1, 0.01)\n",
    "\n",
    "for name, (method, include_last_label) in method_params.items():\n",
    "    mapie = _MapieClassifier(estimator=cirfar10_model, method=method, cv=\"prefit\", random_state=42)\n",
    "    mapie.fit(X_calib, y_calib)\n",
    "    y_preds[name], y_pss[name] = mapie.predict(X_test, alpha=alphas, include_last_label=include_last_label)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now estimate the number of null prediction sets, marginal coverages, and averaged prediction set sizes obtained with the different methods for all confidence levels and for a confidence level of 90 \\%."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def count_null_set(y: np.ndarray) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of empty prediction sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y: np.ndarray of shape (n_sample, )\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "    \"\"\"\n",
    "    count = 0\n",
    "    for pred in y[:, :]:\n",
    "        if np.sum(pred) == 0:\n",
    "            count += 1\n",
    "    return count\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "nulls, coverages, accuracies, sizes = {}, {}, {}, {}\n",
    "for name, (method, include_last_label) in method_params.items():\n",
    "    accuracies[name] = accuracy_score(y_true, y_preds[name])\n",
    "    nulls[name] = [\n",
    "        count_null_set(y_pss[name][:, :, i])  for i, _ in enumerate(alphas)\n",
    "    ]\n",
    "    coverages[name] = [\n",
    "        classification_coverage_score(\n",
    "            y_test, y_pss[name][:, :, i]\n",
    "        ) for i, _ in enumerate(alphas)\n",
    "    ]\n",
    "    sizes[name] = [\n",
    "        y_pss[name][:, :, i].sum(axis=1).mean() for i, _ in enumerate(alphas)\n",
    "    ]\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "coverage_90 = {method: coverage[9] for method, coverage in coverages.items()}\n",
    "null_90 = {method: null[9] for method, null in nulls.items()}\n",
    "width_90 = {method: width[9] for method, width in sizes.items()}\n",
    "y_ps_90 = {method: y_ps[:, :, 9] for method, y_ps in y_pss.items()}"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Let's now look at the marginal coverages, number of null prediction sets, and the averaged size of prediction sets for a confidence level of 90 \\%. "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "summary_df = pd.concat(\n",
    "    [\n",
    "        pd.Series(coverage_90),\n",
    "        pd.Series(null_90),\n",
    "        pd.Series(width_90)\n",
    "    ],\n",
    "    axis=1,\n",
    "    keys=[\"Coverages\", \"Number of null sets\", \"Average prediction set sizes\"]\n",
    ").round(3)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "summary_df"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "As expected, the \"naive\" method, which directly uses the alpha value as a threshold for selecting the prediction sets, does not give guarantees on the marginal coverage since this method is not calibrated. Other methods give a marginal coverage close to the desired one, i.e. 90\\%. Notice that the \"aps\" method, which always includes the last label whose cumulated score is above the given quantile, tends to give slightly higher marginal coverages since the prediction sets are slightly too big."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 5. Visualization of the prediction sets"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def prepare_plot(y_methods: Dict[str, Tuple], n_images: int) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Prepare the number and the disposition of the plots according to\n",
    "    the number of images.\n",
    "    \n",
    "    Paramters:\n",
    "    y_methods: Dict[str, Tuple]\n",
    "        Methods we want to compare.\n",
    "    \n",
    "    n_images: int\n",
    "        Number of images to plot.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray\n",
    "    \"\"\"\n",
    "    plt.rcParams.update({'font.size': FONT_SIZE})\n",
    "    nrow = len(y_methods.keys())\n",
    "    ncol = n_images\n",
    "    s = 5\n",
    "    f, ax = plt.subplots(ncol, nrow, figsize=(s*nrow, s*ncol))\n",
    "    f.tight_layout(pad=SPACE_IN_SUBPLOTS)\n",
    "    rows = [i for i in y_methods.keys()]\n",
    "    \n",
    "    for x, row in zip(ax[:,0], rows):\n",
    "        x.set_ylabel(row, rotation=90, size='large')\n",
    "\n",
    "    return ax\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_position(y_set: List, label: str, count: int, count_true: int) -> float:\n",
    "    \"\"\"\n",
    "    Return the position of each label according to the number of labels to plot.\n",
    "    \n",
    "    Paramters\n",
    "    ---------\n",
    "    y_set: List\n",
    "        Set of predicted labels for one image.\n",
    "    \n",
    "    label: str\n",
    "        Indice of the true label.\n",
    "        \n",
    "    count: int\n",
    "        Index of the label.\n",
    "    \n",
    "    count_true: int\n",
    "        Total number of labels in the prediction set.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "    \"\"\"\n",
    "    if y_set[label] :\n",
    "        position = - (count_true - count)*SPACE_BETWEEN_LABELS\n",
    "\n",
    "    else:\n",
    "        position = - (count_true + 2 - count)*SPACE_BETWEEN_LABELS\n",
    "\n",
    "    return position\n",
    "\n",
    "\n",
    "def add_text(\n",
    "    ax: np.ndarray, indices: Tuple, position: float,\n",
    "    label_name: str, proba: float, color: str, missing: bool = False\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Add the text to the corresponding image.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ax: np.ndarray\n",
    "        Matrix of the images to plot.\n",
    "    \n",
    "    indices: Tuple\n",
    "        Tuple indicating the indices of the image to put\n",
    "        the text on.\n",
    "    \n",
    "    position: float\n",
    "        Position of the text on the image.\n",
    "    \n",
    "    label_name: str\n",
    "        Name of the label to plot.\n",
    "    \n",
    "    proba: float\n",
    "        Proba associated to this label.\n",
    "    \n",
    "    color: str\n",
    "        Color of the text.\n",
    "    \n",
    "    missing: bool\n",
    "        Whether or not the true label is missing in the\n",
    "        prediction set.\n",
    "        \n",
    "        By default False.\n",
    "    \n",
    "    \"\"\"\n",
    "    if not missing :\n",
    "        text = f\"{label_name} : {proba:.4f}\"\n",
    "    else:\n",
    "        text = f\"True label : {label_name} ({proba:.4f})\"\n",
    "    i, j = indices\n",
    "    ax[i, j].text(\n",
    "        15,\n",
    "        position,\n",
    "        text, \n",
    "        ha=\"center\", va=\"top\", \n",
    "        color=color,\n",
    "        font=\"courier new\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_prediction_sets(\n",
    "    X: np.ndarray, y: np.ndarray,\n",
    "    y_pred_proba: np.ndarray,\n",
    "    y_methods: Dict[str, np.ndarray],\n",
    "    n_images: int, label_names: Dict,\n",
    "    random_state: Union[int, None] = None\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Plot random images with their associated prediction\n",
    "    set for all the required methods.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X: np.ndarray of shape (n_sample, width, height, n_channels)\n",
    "        Array containing images.\n",
    "    \n",
    "    y: np.ndarray of shape (n_samples, )\n",
    "        Labels of the images.\n",
    "        \n",
    "    y_pred_proba: np.ndarray of shape (n_samples, n_labels)\n",
    "        Softmax output of the model.\n",
    "    \n",
    "    y_methods: Dict[str, np.ndarray]\n",
    "        Outputs of the _MapieClassifier with the different\n",
    "        choosen methods.\n",
    "    \n",
    "    n_images: int\n",
    "        Number of images to plot\n",
    "    \n",
    "    random_state: Union[int, None]\n",
    "        Random state to use to choose the images.\n",
    "        \n",
    "        By default None.\n",
    "    \"\"\"\n",
    "    random.seed(random_state)\n",
    "    indices = random.sample(range(len(X)), n_images)\n",
    "\n",
    "    y_true = y[indices]\n",
    "    y_pred_proba = y_pred_proba[indices]\n",
    "    ax = prepare_plot(y_methods, n_images)\n",
    "\n",
    "    for i, method in enumerate(y_methods):\n",
    "        y_sets = y_methods[method][indices]\n",
    "\n",
    "        for j in range(n_images):\n",
    "            y_set = y_sets[j]\n",
    "            img, label= X[indices[j]], y_true[j]\n",
    "\n",
    "            ax[i, j].imshow(img)\n",
    "\n",
    "            count_true = np.sum(y_set)\n",
    "            index_sorted_proba = np.argsort(-y_pred_proba[j])\n",
    "\n",
    "            for count in range(count_true):\n",
    "                index_pred = index_sorted_proba[count]\n",
    "                proba = y_pred_proba[j][index_pred]\n",
    "                label_name = label_names[index_pred]\n",
    "                color = 'green' if index_pred == y_true[j] else 'red'\n",
    "                position = get_position(y_set, label, count, count_true)\n",
    "\n",
    "                add_text(ax, (i, j), position, label_name, proba, color)\n",
    "\n",
    "            if not y_set[label] :\n",
    "                label_name = label_names[label]\n",
    "                proba = y_pred_proba[j][label]\n",
    "                add_text(ax, (i, j), -3, label_name, proba, color= 'orange', missing=True)\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_prediction_sets(X_test, y_test, y_pred_proba, y_ps_90, 5, label_names)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 6. Calibration of the methods"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "In this section, we plot the number of null sets, the marginal coverages, and the prediction set sizes as function of the target coverage level for all conformal methods. "
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "vars_y = [nulls, coverages, sizes]\n",
    "labels_y = [\"Empty prediction sets\", \"Marginal coverage\", \"Set sizes\"]\n",
    "fig, axs = plt.subplots(1, len(vars_y), figsize=(8*len(vars_y), 8))\n",
    "for i, var in enumerate(vars_y):\n",
    "    for name, (method, include_last_label) in method_params.items():\n",
    "        axs[i].plot(1 - alphas, var[name], label=name)\n",
    "        if i == 1:\n",
    "            axs[i].plot([0, 1], [0, 1], ls=\"--\", color=\"k\")\n",
    "    axs[i].set_xlabel(\"Couverture cible : 1 - alpha\")\n",
    "    axs[i].set_ylabel(labels_y[i])\n",
    "    if i == len(vars_y) - 1:\n",
    "        axs[i].legend(fontsize=10, loc=[1, 0])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "The two only methods which are perfectly calibrated for the entire range of alpha values are the \"label\" and \"random_aps\". However, these accurate marginal coverages can only be obtained thanks to the generation of null prediction sets. The compromise between estimating null prediction sets with calibrated coverages or non-empty prediction sets but with larger marginal coverages is entirely up to the user."
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 7. Prediction set sizes"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "s=5\n",
    "fig, axs = plt.subplots(1, len(y_preds), figsize=(s*len(y_preds), s))\n",
    "for i, (method, y_ps) in enumerate(y_ps_90.items()):\n",
    "    sizes = y_ps.sum(axis=1)\n",
    "    axs[i].hist(sizes)\n",
    "    axs[i].set_xlabel(\"Prediction set sizes\")\n",
    "    axs[i].set_title(method)"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## 8. Conditional coverages"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We just saw that all our methods (except the \"naive\" one) give marginal coverages always larger than the target coverages for alpha values ranging between 0 and 1. However, there is no mathematical guarantees on the *conditional* coverages, i.e. the coverage obtained for a specific class of images. Let's see what conditional coverages we obtain with the different conformal methods."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def get_class_coverage(\n",
    "    y_test: np.ndarray,\n",
    "    y_method: Dict[str, np.ndarray],\n",
    "    label_names: List[str]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Compute the coverage for each class. As MAPIE is looking for a\n",
    "    global coverage of 1-alpha, it is important to check that their\n",
    "    is not major coverage difference between classes.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_test: np.ndarray of shape (n_samples,)\n",
    "        Labels of the predictions.\n",
    "    \n",
    "    y_method: Dict[str, np.ndarray]\n",
    "        Prediction sets for each method.\n",
    "    \n",
    "    label_names: List[str]\n",
    "        Names of the labels.\n",
    "    \"\"\"\n",
    "    recap ={}\n",
    "    for method in y_method:\n",
    "        recap[method] = []\n",
    "        for label in sorted(np.unique(y_test)):\n",
    "            indices = np.where(y_test==label)\n",
    "            label_name = label_names[label]\n",
    "            y_test_trunc = y_test[indices]\n",
    "            y_set_trunc = y_method[method][indices]\n",
    "            score_coverage = classification_coverage_score(y_test_trunc, y_set_trunc)\n",
    "            recap[method].append(score_coverage)\n",
    "    recap_df = pd.DataFrame(recap, index = label_names)\n",
    "    return recap_df\n",
    "            "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "class_coverage = get_class_coverage(y_test, y_ps_90, label_names)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "fig = plt.figure()\n",
    "class_coverage.plot.bar(figsize=(12, 4), alpha=0.7)\n",
    "plt.axhline(0.9, ls=\"--\", color=\"k\")\n",
    "plt.ylabel(\"Conditional coverage\")\n",
    "plt.legend(loc=[1, 0])"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We can notice that the conditional coverages slightly vary between classes. The only method whose conditional coverages remain valid for all classes is the \"top_k\" one. However, those variations are much smaller than that of the naive method."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def create_confusion_matrix(y_ps: np.ndarray, y_true: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Create a confusion matrix to visualize, for each class, which\n",
    "    classes are which are the most present classes in the prediction\n",
    "    sets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    y_ps: np.ndarray of shape (n_samples, n_labels)\n",
    "        Prediction sets of a specific method.\n",
    "    \n",
    "    y_true: np.ndarray of shape (n_samples, )\n",
    "        Labels of the sample\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray of shape (n_labels, n_labels)\n",
    "    \"\"\"\n",
    "    number_of_classes = len(np.unique(y_true))\n",
    "    confusion_matrix = np.zeros((number_of_classes, number_of_classes))\n",
    "    for i, ps in enumerate(y_ps):\n",
    "        confusion_matrix[y_true[i]] += ps\n",
    "    \n",
    "    return confusion_matrix\n",
    "    "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def reorder_labels(ordered_labels: List, labels: List, cm: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Used to order the labels in the confusion matrix\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    ordered_labels: List\n",
    "        Order you want to have in your confusion matrix\n",
    "    \n",
    "    labels: List\n",
    "        Initial order of the confusion matrix\n",
    "    \n",
    "    cm: np.ndarray of shape (n_labels, n_labels)\n",
    "        Original confusion matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    np.ndarray of shape (n_labels, n_labels)\n",
    "    \"\"\"\n",
    "    cm_ordered = np.zeros(cm.shape)\n",
    "    index_order = [labels.index(label) for label in ordered_labels]\n",
    "    for i, label in enumerate(ordered_labels):\n",
    "        old_index = labels.index(label)\n",
    "        \n",
    "        cm_ordered[i] = cm[old_index, index_order]\n",
    "    return cm_ordered"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def plot_confusion_matrix(method: str, y_ps: Dict[str, np.ndarray], label_names: List) -> None:\n",
    "    \"\"\"\n",
    "    Plot the confusion matrix for a specific method.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    method: str\n",
    "        Name of the method to plot.\n",
    "    \n",
    "    y_ps: Dict[str, np.ndarray]\n",
    "        Prediction sets for each of the fitted method\n",
    "    \n",
    "    label_names: List\n",
    "        Name of the labels\n",
    "    \"\"\"\n",
    "\n",
    "    y_method = y_ps[method]\n",
    "    cm = create_confusion_matrix(y_method, y_test)\n",
    "    ordered_labels = [\"frog\", \"cat\", \"dog\", \"deer\", \"horse\", \"bird\", \"airplane\", \"ship\", \"truck\", \"automobile\"]\n",
    "    cm = reorder_labels(ordered_labels, label_names, cm)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ordered_labels)\n",
    "    _, ax = plt.subplots(figsize=(10, 10))\n",
    "    disp.plot(\n",
    "        include_values=True,\n",
    "        cmap=\"viridis\",\n",
    "        ax=ax,\n",
    "        xticks_rotation=\"vertical\",\n",
    "        values_format='.0f',\n",
    "        colorbar=True,\n",
    "    )\n",
    "\n",
    "    ax.set_title(f'Confusion matrix for {method} method')"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "plot_confusion_matrix(\"aps\", y_ps_90, label_names)"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Thanks to this confusion matrix we can see that, for some labels (as cat, deer and dog) the distribution of the labels in the prediction set is not uniform. Indeed, when the image is a cat, there are almost as many predictions sets with the true label as with the \"cat\" label. In this case, the reverse is also true. However, for the deer, the cat label is often included within the prediction set while the deer is not."
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "c701d105863f3b19d95155354c5cd7eba8f6824e73339ef8c56a1f0753fbe4df"
  },
  "jupytext": {
   "formats": "ipynb,md"
  },
  "kernelspec": {
   "display_name": "mapie-notebooks",
   "language": "python",
   "name": "mapie-notebooks"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
